{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliArabi55/NLP/blob/main/1_Tokenization_and_Text_Preprocessing_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ODlEqyolBe51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7eqGPa19J-3"
      },
      "source": [
        "# Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8FQF39E9J-9"
      },
      "source": [
        "Natural Language Toolkit (NLTK) is a powerful Python library that aids in natural language processing tasks. It was developed at the University of Pennsylvania and has become one of the most popular and widely used libraries in NLP.\n",
        "\n",
        "NLTK provides a wide range of functionalities and resources for text processing, tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and much more. It also includes various corpora, lexical resources, and pre-trained models to help you get started quickly.\n",
        "\n",
        "To begin using NLTK in your Python environment, you need to install it first. The installation process is straightforward and can be done using pip, the standard package manager for Python. Open your command prompt or terminal and run the following command:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks"
      ],
      "metadata": {
        "id": "HfNLkyjYBtU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Tokenize the following text into words using space as a delimiter:\n",
        "Text: \"Natural Language Processing is an exciting field of AI.\"\n",
        "\n",
        "Expected Output:\n",
        "['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI.']\n",
        "=============================================================================\n",
        "Q2: Tokenize the following text into sentences:\n",
        "Text: \"Tokenization is the first step in preprocessing. It helps break text into smaller pieces.\"\n",
        "Expected Output:\n",
        "['Tokenization is the first step in preprocessing.', 'It helps break text into smaller pieces.']\n",
        "==============================================================================\n",
        "Q3 Perform part-of-speech tagging on the following sentence using the NLTK library:\n",
        "Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "==============================================================================\n",
        "Q4: Write a function that accepts a sentence as input and returns the number of nouns and verbs in the sentence after performing POS tagging."
      ],
      "metadata": {
        "id": "QZj6KT02BrDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hyqbl_Z_9J--"
      },
      "outputs": [],
      "source": [
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XtBdVupN9J_B"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJyYdDU29J_B"
      },
      "source": [
        "After importing NLTK, you may want to download additional resources like corpora or models depending on your requirements. NLTK provides a convenient way to download these resources using the nltk.download() function.\n",
        "\n",
        "To download all the available resources at once, you can run:\n",
        "Note: Alternatively, you can choose specific resources to download by replacing 'all' with their respective identifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PhkwUffA9J_C"
      },
      "outputs": [],
      "source": [
        "#nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTLnx4MK9J_D"
      },
      "source": [
        "# Tokenization and Text Preprocessing with NLTK and Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWvXxs689J_E"
      },
      "source": [
        "NLTKâ€™s word tokenization allows you to split text into individual words or tokens. This process is essential for analyzing the linguistic structure of a sentence and extracting meaningful information from it. NLTK provides different tokenization methods, including the default word_tokenize() function and alternative options like TreebankWordTokenizer and RegexpTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XONMxqkq9J_G",
        "outputId": "b2bc2956-40fb-470e-ba30-9495ce344301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources (only required once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Sample text for demonstration\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1"
      ],
      "metadata": {
        "id": "5nB-YwLp-9Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1\n",
        "text = \"Natural Language Processing is an exciting field of AI.\""
      ],
      "metadata": {
        "id": "UgzvMRW3-6_O"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "J-cALwF09J_J",
        "outputId": "93108904-af49-43d8-a322-03e109c391b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tokenization - Word Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\")\n",
        "print(tokens)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2"
      ],
      "metadata": {
        "id": "-rsNgrH5_FG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Task 2\n",
        "text = \"Tokenization is the first step in preprocessing. It helps break text into smaller pieces.\""
      ],
      "metadata": {
        "id": "D8N7aq3Y_Fo-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XvorPjxs9J_K",
        "outputId": "44b9583e-666f-4692-ab19-4e66afa2c192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "['Tokenization is the first step in preprocessing.', 'It helps break text into smaller pieces.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenization - Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "dG9oQmlU9J_M",
        "outputId": "b1b8e091-609c-4505-899d-00e66644bbb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after removing stop words:\n",
            "['Natural', 'Language', 'Processing', 'exciting', 'field', 'AI', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Text Preprocessing - Removing Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.casefold() not in stop_words]\n",
        "print(\"Tokens after removing stop words:\")\n",
        "print(filtered_tokens)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "o_MQegog9J_N",
        "outputId": "64579223-4a6b-49f9-df3d-2ecf6337cdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens:\n",
            "['natur', 'languag', 'process', 'excit', 'field', 'ai', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Text Preprocessing - Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(stemmed_tokens)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Dt4V5spb9J_O",
        "outputId": "8d9680c0-f095-4ed8-d069-26a1a4d8427f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens:\n",
            "['Natural', 'Language', 'Processing', 'exciting', 'field', 'AI', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Text Preprocessing - Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(\"Lemmatized Tokens:\")\n",
        "print(lemmatized_tokens)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "29koiCQk9J_O",
        "outputId": "af94d989-e89f-4936-a6c1-ddf3cbc30b7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after handling special characters:\n",
            "['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI']\n"
          ]
        }
      ],
      "source": [
        "# Text Preprocessing - Handling Special Characters\n",
        "special_chars = set(string.punctuation)\n",
        "filtered_tokens = [token for token in tokens if token not in special_chars]\n",
        "print(\"Tokens after handling special characters:\")\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n"
      ],
      "metadata": {
        "id": "v3EynD_h99pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3 Perform part-of-speech tagging on the following sentence using the NLTK library:\n",
        "### Sentence: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "4_UMEMaR-Jo2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txNZZ1Np9J_P"
      },
      "source": [
        "# Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "h-ZJoNPk9J_Q",
        "outputId": "af356297-bad2-4d54-d431-601ebae30b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The: DT\n",
            "quick: JJ\n",
            "brown: NN\n",
            "fox: NN\n",
            "jumps: VBZ\n",
            "over: IN\n",
            "the: DT\n",
            "lazy: JJ\n",
            "dog: NN\n",
            ".: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources (only required once)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text for demonstration\n",
        "\n",
        "#Task 3\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "\n",
        "#Task 4\n",
        "\n",
        "#text=\"Write a function that accepts a sentence as input and returns the number of nouns and verbs in the sentence after performing POS tagging. \"\n",
        "\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the POS tags\n",
        "for token, pos_tag in pos_tags:\n",
        "    print(f\"{token}: {pos_tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "IPs6QnFBB18f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nouns_verb_retreiver(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    nouns = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
        "    verbs = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
        "    return nouns, verbs\n",
        "\n",
        "nouns, verbs = nouns_verb_retreiver(text)\n",
        "print(f\"nouns = {nouns}, verbs = {verbs}\")\n"
      ],
      "metadata": {
        "id": "PAob25ETAPlE",
        "outputId": "1c01a339-0548-4529-9398-7d8ac855fc58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns = 3, verbs = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdBTxUsaAgbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}