{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVz1udv4iW0e79jVusew6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliArabi55/NLP/blob/main/NLP_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objective:**"
      ],
      "metadata": {
        "id": "sSkIeVMsYjHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab is designed to help students understand the steps involved in cleaning text data, performing linguistic analysis, and extracting features using TF-IDF and N-gram models. By the end of the lab, students will have hands-on experience in preparing text data for machine learning tasks, such as text classification or sentiment analysis."
      ],
      "metadata": {
        "id": "qLj4FCx7YmIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries Installation**"
      ],
      "metadata": {
        "id": "VD-UQZS2YohZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4tozO0qYQSh",
        "outputId": "1ba3543e-cc7f-4b6a-95ee-0b1a45145dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Tokenization✅**"
      ],
      "metadata": {
        "id": "mtd7qfoVYzBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\n",
        "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
        "    \"I hated the film, it was the worst I have ever seen\",\n",
        "    \"The storyline was boring but the acting was brilliant\",\n",
        "    \"An amazing movie with a great plot and incredible performances\",\n",
        "    \"Egypt movie, I regret wasting my time on it\",\n",
        "    \"The actors did a great job but the story lacked depth\",\n",
        "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
        "    \"This film was just okay, not too bad but not great either\",\n",
        "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
        "    \"The movie was disappointing, it did not live up to the hype\"\n",
        "]\n",
        "\n",
        "# Tokenization Code\n",
        "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKkaw7LBYz3x",
        "outputId": "6f5c8cca-e007-4459-d704-b233b7e3df2b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The',\n",
              "  'movie',\n",
              "  'was',\n",
              "  'fantastic',\n",
              "  'and',\n",
              "  'I',\n",
              "  'loved',\n",
              "  'every',\n",
              "  'part',\n",
              "  'of',\n",
              "  'it',\n",
              "  'about',\n",
              "  'Egypt'],\n",
              " ['I',\n",
              "  'hated',\n",
              "  'the',\n",
              "  'film',\n",
              "  ',',\n",
              "  'it',\n",
              "  'was',\n",
              "  'the',\n",
              "  'worst',\n",
              "  'I',\n",
              "  'have',\n",
              "  'ever',\n",
              "  'seen'],\n",
              " ['The',\n",
              "  'storyline',\n",
              "  'was',\n",
              "  'boring',\n",
              "  'but',\n",
              "  'the',\n",
              "  'acting',\n",
              "  'was',\n",
              "  'brilliant'],\n",
              " ['An',\n",
              "  'amazing',\n",
              "  'movie',\n",
              "  'with',\n",
              "  'a',\n",
              "  'great',\n",
              "  'plot',\n",
              "  'and',\n",
              "  'incredible',\n",
              "  'performances'],\n",
              " ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it'],\n",
              " ['The',\n",
              "  'actors',\n",
              "  'did',\n",
              "  'a',\n",
              "  'great',\n",
              "  'job',\n",
              "  'but',\n",
              "  'the',\n",
              "  'story',\n",
              "  'lacked',\n",
              "  'depth'],\n",
              " ['One',\n",
              "  'of',\n",
              "  'the',\n",
              "  'best',\n",
              "  'films',\n",
              "  'I',\n",
              "  'have',\n",
              "  'seen',\n",
              "  'in',\n",
              "  'a',\n",
              "  'long',\n",
              "  'time',\n",
              "  ',',\n",
              "  'highly',\n",
              "  'recommend',\n",
              "  'it'],\n",
              " ['This',\n",
              "  'film',\n",
              "  'was',\n",
              "  'just',\n",
              "  'okay',\n",
              "  ',',\n",
              "  'not',\n",
              "  'too',\n",
              "  'bad',\n",
              "  'but',\n",
              "  'not',\n",
              "  'great',\n",
              "  'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'the',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'and',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['The',\n",
              "  'movie',\n",
              "  'was',\n",
              "  'disappointing',\n",
              "  ',',\n",
              "  'it',\n",
              "  'did',\n",
              "  'not',\n",
              "  'live',\n",
              "  'up',\n",
              "  'to',\n",
              "  'the',\n",
              "  'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2: Stopword Removal✅**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xproACeOY-iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove Stopwords\n",
        "cleaned_data = []\n",
        "for sentence in tokenized_data:\n",
        "    filtered_sentence = [word for word in sentence if word.lower() not in stop_words]\n",
        "    cleaned_data.append(filtered_sentence)\n",
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PBSzgSqZpoF",
        "outputId": "52ce08bb-6406-4469-f84f-c70ead705bc8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actors', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'films', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 3: Stemming or Lemmatization✅**\n",
        "  "
      ],
      "metadata": {
        "id": "TzgqVDVvZrmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_data = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in cleaned_data]\n",
        "lemmatized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4MHAfrjpVjP",
        "outputId": "f1fc8d83-a091-4fef-e0a0-5d62b3907cd3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performance'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actor', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'film', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 4: Part-of-Speech (POS) Tagging✅**\n"
      ],
      "metadata": {
        "id": "CBp4anINaDWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_data = [nltk.pos_tag(sentence) for sentence in tokenized_data]\n",
        "pos_tagged_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrC3LoYXaGfw",
        "outputId": "a2ca464c-7652-4ad7-e200-5fee5cce8c6a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('and', 'CC'),\n",
              "  ('I', 'PRP'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('every', 'DT'),\n",
              "  ('part', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('it', 'PRP'),\n",
              "  ('about', 'IN'),\n",
              "  ('Egypt', 'NNP')],\n",
              " [('I', 'PRP'),\n",
              "  ('hated', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('was', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('worst', 'JJS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('ever', 'RB'),\n",
              "  ('seen', 'VBN')],\n",
              " [('The', 'DT'),\n",
              "  ('storyline', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('boring', 'VBG'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('acting', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('brilliant', 'JJ')],\n",
              " [('An', 'DT'),\n",
              "  ('amazing', 'JJ'),\n",
              "  ('movie', 'NN'),\n",
              "  ('with', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('incredible', 'JJ'),\n",
              "  ('performances', 'NNS')],\n",
              " [('Egypt', 'NNP'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('I', 'PRP'),\n",
              "  ('regret', 'VBP'),\n",
              "  ('wasting', 'VBG'),\n",
              "  ('my', 'PRP$'),\n",
              "  ('time', 'NN'),\n",
              "  ('on', 'IN'),\n",
              "  ('it', 'PRP')],\n",
              " [('The', 'DT'),\n",
              "  ('actors', 'NNS'),\n",
              "  ('did', 'VBD'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('job', 'NN'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('story', 'NN'),\n",
              "  ('lacked', 'VBD'),\n",
              "  ('depth', 'NN')],\n",
              " [('One', 'CD'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'DT'),\n",
              "  ('best', 'JJS'),\n",
              "  ('films', 'NNS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('seen', 'VBN'),\n",
              "  ('in', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('long', 'JJ'),\n",
              "  ('time', 'NN'),\n",
              "  (',', ','),\n",
              "  ('highly', 'RB'),\n",
              "  ('recommend', 'VB'),\n",
              "  ('it', 'PRP')],\n",
              " [('This', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('just', 'RB'),\n",
              "  ('okay', 'RB'),\n",
              "  (',', ','),\n",
              "  ('not', 'RB'),\n",
              "  ('too', 'RB'),\n",
              "  ('bad', 'JJ'),\n",
              "  ('but', 'CC'),\n",
              "  ('not', 'RB'),\n",
              "  ('great', 'JJ'),\n",
              "  ('either', 'CC')],\n",
              " [('Absolutely', 'RB'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('wonderful', 'JJ'),\n",
              "  ('cast', 'NN')],\n",
              " [('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('disappointing', 'JJ'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('did', 'VBD'),\n",
              "  ('not', 'RB'),\n",
              "  ('live', 'VB'),\n",
              "  ('up', 'RB'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('hype', 'NN')]]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 5: Named Entity Recognition (NER)✅**\n"
      ],
      "metadata": {
        "id": "cAjrooiEaPrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_data = [nltk.ne_chunk(pos_tagged) for pos_tagged in pos_tagged_data]\n",
        "ner_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktaUKA7LaRMA",
        "outputId": "25d4c314-5292-4570-f02f-9278ccb757e6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('S', [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('fantastic', 'JJ'), ('and', 'CC'), ('I', 'PRP'), ('loved', 'VBD'), ('every', 'DT'), ('part', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('about', 'IN'), Tree('GPE', [('Egypt', 'NNP')])]),\n",
              " Tree('S', [('I', 'PRP'), ('hated', 'VBD'), ('the', 'DT'), ('film', 'NN'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('worst', 'JJS'), ('I', 'PRP'), ('have', 'VBP'), ('ever', 'RB'), ('seen', 'VBN')]),\n",
              " Tree('S', [('The', 'DT'), ('storyline', 'NN'), ('was', 'VBD'), ('boring', 'VBG'), ('but', 'CC'), ('the', 'DT'), ('acting', 'NN'), ('was', 'VBD'), ('brilliant', 'JJ')]),\n",
              " Tree('S', [('An', 'DT'), ('amazing', 'JJ'), ('movie', 'NN'), ('with', 'IN'), ('a', 'DT'), ('great', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('incredible', 'JJ'), ('performances', 'NNS')]),\n",
              " Tree('S', [Tree('GPE', [('Egypt', 'NNP')]), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('regret', 'VBP'), ('wasting', 'VBG'), ('my', 'PRP$'), ('time', 'NN'), ('on', 'IN'), ('it', 'PRP')]),\n",
              " Tree('S', [('The', 'DT'), ('actors', 'NNS'), ('did', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('job', 'NN'), ('but', 'CC'), ('the', 'DT'), ('story', 'NN'), ('lacked', 'VBD'), ('depth', 'NN')]),\n",
              " Tree('S', [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('films', 'NNS'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('long', 'JJ'), ('time', 'NN'), (',', ','), ('highly', 'RB'), ('recommend', 'VB'), ('it', 'PRP')]),\n",
              " Tree('S', [('This', 'DT'), ('film', 'NN'), ('was', 'VBD'), ('just', 'RB'), ('okay', 'RB'), (',', ','), ('not', 'RB'), ('too', 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('not', 'RB'), ('great', 'JJ'), ('either', 'CC')]),\n",
              " Tree('S', [('Absolutely', 'RB'), ('loved', 'VBD'), ('the', 'DT'), ('movie', 'NN'), (',', ','), ('fantastic', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('wonderful', 'JJ'), ('cast', 'NN')]),\n",
              " Tree('S', [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('disappointing', 'JJ'), (',', ','), ('it', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('live', 'VB'), ('up', 'RB'), ('to', 'TO'), ('the', 'DT'), ('hype', 'NN')])]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 6: TF-IDF✅**\n"
      ],
      "metadata": {
        "id": "vZYa5dETasrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "tfidf_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHTH2ekbaxxZ",
        "outputId": "dbd86b21-3230-4508-b35e-ab42d831bf73"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35946021, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26734116, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30557402, 0.        , 0.        , 0.35946021,\n",
              "        0.30557402, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.21345497, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30557402, 0.21345497, 0.        , 0.        ,\n",
              "        0.30557402, 0.        , 0.        , 0.        , 0.35946021,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.17522211, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.21345497, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.3828166 , 0.        ,\n",
              "        0.        , 0.32542908, 0.        , 0.        , 0.3828166 ,\n",
              "        0.32542908, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.22732448, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.32542908,\n",
              "        0.        , 0.        , 0.37321477, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.22732448, 0.        ,\n",
              "        0.        , 0.        , 0.3828166 ],\n",
              "       [0.        , 0.        , 0.38030536, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.38030536,\n",
              "        0.38030536, 0.28284431, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.38030536, 0.37076652, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.4516665 , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.37315652,\n",
              "        0.37315652, 0.27752751, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.27752751, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.37315652,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.22158812, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.37315652, 0.31721714, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.37315652, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.3427744 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23944083, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23944083, 0.40322066, 0.        ,\n",
              "        0.        , 0.        , 0.40322066, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.40322066, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.3427744 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.40322066,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.35853148, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26665044, 0.        , 0.35853148, 0.30478452,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.26665044, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.35853148, 0.        , 0.35853148, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.35853148, 0.        , 0.34953878, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.30888835, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30888835, 0.        , 0.        ,\n",
              "        0.26258332, 0.30888835, 0.        , 0.30888835, 0.        ,\n",
              "        0.18342434, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.30888835, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.26258332, 0.        , 0.        , 0.30888835, 0.        ,\n",
              "        0.        , 0.        , 0.30888835, 0.        , 0.26258332,\n",
              "        0.        , 0.        , 0.1505704 , 0.        , 0.26258332,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.22351089, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.25547552, 0.        , 0.22351089, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.51095104,\n",
              "        0.        , 0.30052713, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.30052713, 0.        ,\n",
              "        0.        , 0.30052713, 0.        , 0.17845928, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.39805148, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.29604263, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.39805148, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.33838013, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.33838013, 0.23637127, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.33838013, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19403377, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.39805148, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.29237726,\n",
              "        0.34393628, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.34393628, 0.        , 0.        ,\n",
              "        0.20423653, 0.        , 0.        , 0.        , 0.34393628,\n",
              "        0.        , 0.        , 0.20423653, 0.        , 0.29237726,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.33530965, 0.        , 0.        ,\n",
              "        0.34393628, 0.        , 0.34393628, 0.20423653, 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 7: N-gram Extraction✅**\n"
      ],
      "metadata": {
        "id": "V2zlrreha3xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: N-gram Extraction\n",
        "\n",
        "# Import necessary library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define N-gram range for bigrams (n=2)\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Apply N-gram extraction on the text data\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Convert the matrix to an array and print it\n",
        "bigram_array = bigram_matrix.toarray()\n",
        "bigram_array\n",
        "\n",
        "# To view the feature names (i.e., the actual bigrams)\n",
        "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
        "print(bigram_features)\n",
        "\n",
        "# Example of predicting the next word using N-gram probabilities (as in your provided file)\n",
        "def predict_next_word(model, context, n=2):\n",
        "    # Code to predict next word using an N-gram model\n",
        "    # For simplicity, return a dummy word\n",
        "    return \"example_next_word\"\n",
        "\n",
        "# Context sentence for prediction\n",
        "context = [\"machine\", \"learning\"]\n",
        "\n",
        "# Example usage of the N-gram model for word prediction\n",
        "predicted_word = predict_next_word(bigram_vectorizer, context, n=2) # Changed bigram_model to bigram_vectorizer\n",
        "print(f\"Predicted next word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2WRxL-Oa4z4",
        "outputId": "b1ca1ec4-c911-45a8-eee0-5a0ba856ef05"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about egypt' 'absolutely loved' 'acting was' 'actors did'\n",
            " 'amazing movie' 'an amazing' 'and incredible' 'and loved' 'and wonderful'\n",
            " 'bad but' 'best films' 'boring but' 'but not' 'but the' 'did great'\n",
            " 'did not' 'disappointing it' 'egypt movie' 'ever seen' 'every part'\n",
            " 'fantastic and' 'fantastic plot' 'film it' 'film was' 'films have'\n",
            " 'great either' 'great job' 'great plot' 'hated the' 'have ever'\n",
            " 'have seen' 'highly recommend' 'in long' 'incredible performances'\n",
            " 'it about' 'it did' 'it was' 'job but' 'just okay' 'lacked depth'\n",
            " 'live up' 'long time' 'loved every' 'loved the' 'movie fantastic'\n",
            " 'movie regret' 'movie was' 'movie with' 'my time' 'not great' 'not live'\n",
            " 'not too' 'of it' 'of the' 'okay not' 'on it' 'one of' 'part of'\n",
            " 'plot and' 'recommend it' 'regret wasting' 'seen in' 'story lacked'\n",
            " 'storyline was' 'the acting' 'the actors' 'the best' 'the film'\n",
            " 'the hype' 'the movie' 'the story' 'the storyline' 'the worst'\n",
            " 'this film' 'time highly' 'time on' 'to the' 'too bad' 'up to'\n",
            " 'was boring' 'was brilliant' 'was disappointing' 'was fantastic'\n",
            " 'was just' 'was the' 'wasting my' 'with great' 'wonderful cast'\n",
            " 'worst have']\n",
            "Predicted next word: example_next_word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 8 Exploratory Questions on Task 4 & 5✅**\n"
      ],
      "metadata": {
        "id": "iwOF9nY6dd2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1-What types of entities (e.g., people, places, organizations) are most commonly identified in your text data?**"
      ],
      "metadata": {
        "id": "8hEXCHoBd-mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Based on the Named Entity Recognition (NER) output from Task 5, analyze the results to see which types of entities are most frequent. For example, in the given movie review dataset, entities like \"Egypt\" (place) and possible people names or organizations related to movies could appear.\n"
      ],
      "metadata": {
        "id": "Xx3kWxeBeYbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2-How do these entities contribute to the overall meaning of the document?**\n"
      ],
      "metadata": {
        "id": "FrAjy_r6d87D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Entities like locations or organizations often provide important contextual information. In movie reviews, for example, if \"Egypt\" is mentioned multiple times, it may indicate a theme related to Egyptian movies, or movies filmed in Egypt, which influences the subject matter and tone of the review.\n"
      ],
      "metadata": {
        "id": "j5z16XPTezLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3-After performing POS tagging on your text, which parts of speech (e.g., nouns, verbs, adjectives) appear most frequently in the movie reviews?**"
      ],
      "metadata": {
        "id": "CWKiXBKce7so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Using POS tagging (Task 4), calculate the frequency of different parts of speech. Movie reviews tend to use a lot of adjectives (e.g., 'fantastic', 'boring') to express opinions, as well as nouns and verbs. High adjective frequency often suggests that the reviews are opinion-heavy, which is typical for sentiment-related text.\n"
      ],
      "metadata": {
        "id": "ITg-e5YRfBAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 9 Questions on Tasks 6 (TF-IDF) and 7 (N-gram)✅**"
      ],
      "metadata": {
        "id": "kLOGlTPudjLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1-What is the main purpose of applying TF-IDF in text processing?**\n"
      ],
      "metadata": {
        "id": "k3-F4vJRfzK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The purpose of applying TF-IDF is to convert text into numerical features, so that it reflects how important a word is to a document within a collection (corpus). TF-IDF helps emphasize words that are more relevant to a specific document but are less common across the whole corpus, providing better input for machine learning models.\n"
      ],
      "metadata": {
        "id": "_UMp3jFqgVOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2-How does the inverse document frequency (IDF) part of TF-IDF affect the importance of a word?**\n"
      ],
      "metadata": {
        "id": "O_HiQcXPf3Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The IDF part of TF-IDF reduces the influence of commonly used words (e.g., 'the', 'is') by assigning them lower importance. Words that appear in many documents receive a lower weight, while rare but relevant words receive a higher weight, helping to distinguish documents based on unique terms.\n"
      ],
      "metadata": {
        "id": "XM2Ks3OBgW7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3-What are the possible issues that can arise if you do not apply stopword removal before calculating TF-IDF? (see my your experiment )**\n"
      ],
      "metadata": {
        "id": "whMZExoqf7TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####If stopword removal is not applied, common words that carry little meaning (e.g., 'the', 'and') will be assigned high TF-IDF values, despite not contributing much to the document's unique content. This could distort the feature set by giving irrelevant words higher importance, affecting model performance.\n"
      ],
      "metadata": {
        "id": "-Xjlub2KgaLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4-What is the difference between unigrams, bigrams, and trigrams?**\n"
      ],
      "metadata": {
        "id": "dD1HtfIAf-VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####- **Unigrams:** Single words extracted from the text.\n",
        "####- **Bigrams:** Consecutive sequences of two words (e.g., 'great movie').\n",
        "####- **Trigrams:** Sequences of three consecutive words (e.g., 'the great movie').\n",
        "\n",
        "####Each type captures a different level of context:\n",
        "####- **Unigrams** focus on individual word importance.\n",
        "####- **Bigrams** and **trigrams** consider word pairs or triplets, which are useful for capturing short phrases or dependencies between words.\n"
      ],
      "metadata": {
        "id": "HPVnewpTgdXB"
      }
    }
  ]
}