{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sSkIeVMsYjHi",
        "mtd7qfoVYzBA",
        "xproACeOY-iA",
        "TzgqVDVvZrmY",
        "whMZExoqf7TT"
      ],
      "authorship_tag": "ABX9TyO8N4KOa3hq9TxPDBRYbuxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliArabi55/NLP/blob/main/NLP_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objective:**"
      ],
      "metadata": {
        "id": "sSkIeVMsYjHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab is designed to help students understand the steps involved in cleaning text data, performing linguistic analysis, and extracting features using TF-IDF and N-gram models. By the end of the lab, students will have hands-on experience in preparing text data for machine learning tasks, such as text classification or sentiment analysis."
      ],
      "metadata": {
        "id": "qLj4FCx7YmIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries Installation**"
      ],
      "metadata": {
        "id": "VD-UQZS2YohZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4tozO0qYQSh",
        "outputId": "86651d02-ebe5-4b62-e5ff-d3492af1e73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Tokenizationâœ…**"
      ],
      "metadata": {
        "id": "mtd7qfoVYzBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\n",
        "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
        "    \"I hated the film, it was the worst I have ever seen\",\n",
        "    \"The storyline was boring but the acting was brilliant\",\n",
        "    \"An amazing movie with a great plot and incredible performances\",\n",
        "    \"Egypt movie, I regret wasting my time on it\",\n",
        "    \"The actors did a great job but the story lacked depth\",\n",
        "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
        "    \"This film was just okay, not too bad but not great either\",\n",
        "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
        "    \"The movie was disappointing, it did not live up to the hype\"\n",
        "]\n",
        "\n",
        "# Tokenization Code\n",
        "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKkaw7LBYz3x",
        "outputId": "c29b980c-a20c-4e3c-9cb7-0d453aee8a99"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The',\n",
              "  'movie',\n",
              "  'was',\n",
              "  'fantastic',\n",
              "  'and',\n",
              "  'I',\n",
              "  'loved',\n",
              "  'every',\n",
              "  'part',\n",
              "  'of',\n",
              "  'it',\n",
              "  'about',\n",
              "  'Egypt'],\n",
              " ['I',\n",
              "  'hated',\n",
              "  'the',\n",
              "  'film',\n",
              "  ',',\n",
              "  'it',\n",
              "  'was',\n",
              "  'the',\n",
              "  'worst',\n",
              "  'I',\n",
              "  'have',\n",
              "  'ever',\n",
              "  'seen'],\n",
              " ['The',\n",
              "  'storyline',\n",
              "  'was',\n",
              "  'boring',\n",
              "  'but',\n",
              "  'the',\n",
              "  'acting',\n",
              "  'was',\n",
              "  'brilliant'],\n",
              " ['An',\n",
              "  'amazing',\n",
              "  'movie',\n",
              "  'with',\n",
              "  'a',\n",
              "  'great',\n",
              "  'plot',\n",
              "  'and',\n",
              "  'incredible',\n",
              "  'performances'],\n",
              " ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it'],\n",
              " ['The',\n",
              "  'actors',\n",
              "  'did',\n",
              "  'a',\n",
              "  'great',\n",
              "  'job',\n",
              "  'but',\n",
              "  'the',\n",
              "  'story',\n",
              "  'lacked',\n",
              "  'depth'],\n",
              " ['One',\n",
              "  'of',\n",
              "  'the',\n",
              "  'best',\n",
              "  'films',\n",
              "  'I',\n",
              "  'have',\n",
              "  'seen',\n",
              "  'in',\n",
              "  'a',\n",
              "  'long',\n",
              "  'time',\n",
              "  ',',\n",
              "  'highly',\n",
              "  'recommend',\n",
              "  'it'],\n",
              " ['This',\n",
              "  'film',\n",
              "  'was',\n",
              "  'just',\n",
              "  'okay',\n",
              "  ',',\n",
              "  'not',\n",
              "  'too',\n",
              "  'bad',\n",
              "  'but',\n",
              "  'not',\n",
              "  'great',\n",
              "  'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'the',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'and',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['The',\n",
              "  'movie',\n",
              "  'was',\n",
              "  'disappointing',\n",
              "  ',',\n",
              "  'it',\n",
              "  'did',\n",
              "  'not',\n",
              "  'live',\n",
              "  'up',\n",
              "  'to',\n",
              "  'the',\n",
              "  'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2: Stopword Removalâœ…**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xproACeOY-iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove Stopwords\n",
        "cleaned_data = []\n",
        "for sentence in tokenized_data:\n",
        "    filtered_sentence = [word for word in sentence if word.lower() not in stop_words]\n",
        "    cleaned_data.append(filtered_sentence)\n",
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PBSzgSqZpoF",
        "outputId": "f6cb5022-7da5-4cae-c1e4-5034cc00b83f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actors', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'films', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 3: Stemming or Lemmatizationâœ…**\n",
        "  "
      ],
      "metadata": {
        "id": "TzgqVDVvZrmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_data = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in cleaned_data]\n",
        "lemmatized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4MHAfrjpVjP",
        "outputId": "1c4c6df5-0cda-4a21-975a-460f93cb9684"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performance'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actor', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'film', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 4: Part-of-Speech (POS) Taggingâœ…**\n"
      ],
      "metadata": {
        "id": "CBp4anINaDWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_data = [nltk.pos_tag(sentence) for sentence in tokenized_data]\n",
        "pos_tagged_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrC3LoYXaGfw",
        "outputId": "0354477f-0e44-4dbd-82bb-1c3b6b8834e5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('and', 'CC'),\n",
              "  ('I', 'PRP'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('every', 'DT'),\n",
              "  ('part', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('it', 'PRP'),\n",
              "  ('about', 'IN'),\n",
              "  ('Egypt', 'NNP')],\n",
              " [('I', 'PRP'),\n",
              "  ('hated', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('was', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('worst', 'JJS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('ever', 'RB'),\n",
              "  ('seen', 'VBN')],\n",
              " [('The', 'DT'),\n",
              "  ('storyline', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('boring', 'VBG'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('acting', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('brilliant', 'JJ')],\n",
              " [('An', 'DT'),\n",
              "  ('amazing', 'JJ'),\n",
              "  ('movie', 'NN'),\n",
              "  ('with', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('incredible', 'JJ'),\n",
              "  ('performances', 'NNS')],\n",
              " [('Egypt', 'NNP'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('I', 'PRP'),\n",
              "  ('regret', 'VBP'),\n",
              "  ('wasting', 'VBG'),\n",
              "  ('my', 'PRP$'),\n",
              "  ('time', 'NN'),\n",
              "  ('on', 'IN'),\n",
              "  ('it', 'PRP')],\n",
              " [('The', 'DT'),\n",
              "  ('actors', 'NNS'),\n",
              "  ('did', 'VBD'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('job', 'NN'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('story', 'NN'),\n",
              "  ('lacked', 'VBD'),\n",
              "  ('depth', 'NN')],\n",
              " [('One', 'CD'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'DT'),\n",
              "  ('best', 'JJS'),\n",
              "  ('films', 'NNS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('seen', 'VBN'),\n",
              "  ('in', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('long', 'JJ'),\n",
              "  ('time', 'NN'),\n",
              "  (',', ','),\n",
              "  ('highly', 'RB'),\n",
              "  ('recommend', 'VB'),\n",
              "  ('it', 'PRP')],\n",
              " [('This', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('just', 'RB'),\n",
              "  ('okay', 'RB'),\n",
              "  (',', ','),\n",
              "  ('not', 'RB'),\n",
              "  ('too', 'RB'),\n",
              "  ('bad', 'JJ'),\n",
              "  ('but', 'CC'),\n",
              "  ('not', 'RB'),\n",
              "  ('great', 'JJ'),\n",
              "  ('either', 'CC')],\n",
              " [('Absolutely', 'RB'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('wonderful', 'JJ'),\n",
              "  ('cast', 'NN')],\n",
              " [('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('disappointing', 'JJ'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('did', 'VBD'),\n",
              "  ('not', 'RB'),\n",
              "  ('live', 'VB'),\n",
              "  ('up', 'RB'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('hype', 'NN')]]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 5: Named Entity Recognition (NER)âœ…**\n"
      ],
      "metadata": {
        "id": "cAjrooiEaPrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_data = [nltk.ne_chunk(pos_tagged) for pos_tagged in pos_tagged_data]\n",
        "ner_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktaUKA7LaRMA",
        "outputId": "4cda61d3-d359-4700-a4eb-ef18ef157539"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('S', [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('fantastic', 'JJ'), ('and', 'CC'), ('I', 'PRP'), ('loved', 'VBD'), ('every', 'DT'), ('part', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('about', 'IN'), Tree('GPE', [('Egypt', 'NNP')])]),\n",
              " Tree('S', [('I', 'PRP'), ('hated', 'VBD'), ('the', 'DT'), ('film', 'NN'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('worst', 'JJS'), ('I', 'PRP'), ('have', 'VBP'), ('ever', 'RB'), ('seen', 'VBN')]),\n",
              " Tree('S', [('The', 'DT'), ('storyline', 'NN'), ('was', 'VBD'), ('boring', 'VBG'), ('but', 'CC'), ('the', 'DT'), ('acting', 'NN'), ('was', 'VBD'), ('brilliant', 'JJ')]),\n",
              " Tree('S', [('An', 'DT'), ('amazing', 'JJ'), ('movie', 'NN'), ('with', 'IN'), ('a', 'DT'), ('great', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('incredible', 'JJ'), ('performances', 'NNS')]),\n",
              " Tree('S', [Tree('GPE', [('Egypt', 'NNP')]), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('regret', 'VBP'), ('wasting', 'VBG'), ('my', 'PRP$'), ('time', 'NN'), ('on', 'IN'), ('it', 'PRP')]),\n",
              " Tree('S', [('The', 'DT'), ('actors', 'NNS'), ('did', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('job', 'NN'), ('but', 'CC'), ('the', 'DT'), ('story', 'NN'), ('lacked', 'VBD'), ('depth', 'NN')]),\n",
              " Tree('S', [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('films', 'NNS'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('long', 'JJ'), ('time', 'NN'), (',', ','), ('highly', 'RB'), ('recommend', 'VB'), ('it', 'PRP')]),\n",
              " Tree('S', [('This', 'DT'), ('film', 'NN'), ('was', 'VBD'), ('just', 'RB'), ('okay', 'RB'), (',', ','), ('not', 'RB'), ('too', 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('not', 'RB'), ('great', 'JJ'), ('either', 'CC')]),\n",
              " Tree('S', [('Absolutely', 'RB'), ('loved', 'VBD'), ('the', 'DT'), ('movie', 'NN'), (',', ','), ('fantastic', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('wonderful', 'JJ'), ('cast', 'NN')]),\n",
              " Tree('S', [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('disappointing', 'JJ'), (',', ','), ('it', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('live', 'VB'), ('up', 'RB'), ('to', 'TO'), ('the', 'DT'), ('hype', 'NN')])]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 6: TF-IDFâœ…**\n"
      ],
      "metadata": {
        "id": "vZYa5dETasrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "tfidf_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHTH2ekbaxxZ",
        "outputId": "f6cdd525-5887-4936-d1e8-f314609ac2e0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35946021, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26734116, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30557402, 0.        , 0.        , 0.35946021,\n",
              "        0.30557402, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.21345497, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30557402, 0.21345497, 0.        , 0.        ,\n",
              "        0.30557402, 0.        , 0.        , 0.        , 0.35946021,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.17522211, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.21345497, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.3828166 , 0.        ,\n",
              "        0.        , 0.32542908, 0.        , 0.        , 0.3828166 ,\n",
              "        0.32542908, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.22732448, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.32542908,\n",
              "        0.        , 0.        , 0.37321477, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.22732448, 0.        ,\n",
              "        0.        , 0.        , 0.3828166 ],\n",
              "       [0.        , 0.        , 0.38030536, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.38030536,\n",
              "        0.38030536, 0.28284431, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.38030536, 0.37076652, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.4516665 , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.37315652,\n",
              "        0.37315652, 0.27752751, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.27752751, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.37315652,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.22158812, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.37315652, 0.31721714, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.37315652, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.3427744 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23944083, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23944083, 0.40322066, 0.        ,\n",
              "        0.        , 0.        , 0.40322066, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.40322066, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.3427744 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.40322066,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.35853148, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26665044, 0.        , 0.35853148, 0.30478452,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.26665044, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.35853148, 0.        , 0.35853148, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.35853148, 0.        , 0.34953878, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.30888835, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30888835, 0.        , 0.        ,\n",
              "        0.26258332, 0.30888835, 0.        , 0.30888835, 0.        ,\n",
              "        0.18342434, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.30888835, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.26258332, 0.        , 0.        , 0.30888835, 0.        ,\n",
              "        0.        , 0.        , 0.30888835, 0.        , 0.26258332,\n",
              "        0.        , 0.        , 0.1505704 , 0.        , 0.26258332,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.22351089, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.25547552, 0.        , 0.22351089, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30052713, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.51095104,\n",
              "        0.        , 0.30052713, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.30052713, 0.        ,\n",
              "        0.        , 0.30052713, 0.        , 0.17845928, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.39805148, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.29604263, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.39805148, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.33838013, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.33838013, 0.23637127, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.33838013, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19403377, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.39805148, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.29237726,\n",
              "        0.34393628, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.34393628, 0.        , 0.        ,\n",
              "        0.20423653, 0.        , 0.        , 0.        , 0.34393628,\n",
              "        0.        , 0.        , 0.20423653, 0.        , 0.29237726,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.33530965, 0.        , 0.        ,\n",
              "        0.34393628, 0.        , 0.34393628, 0.20423653, 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 7: N-gram Extractionâœ…**\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V2zlrreha3xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: N-gram Extraction\n",
        "\n",
        "# Import necessary library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define N-gram range for bigrams (n=2)\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Apply N-gram extraction on the text data\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Convert the matrix to an array and print it\n",
        "bigram_array = bigram_matrix.toarray()\n",
        "bigram_array\n",
        "\n",
        "# To view the feature names (i.e., the actual bigrams)\n",
        "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
        "print(bigram_features)\n",
        "\n",
        "# Example of predicting the next word using N-gram probabilities (as in your provided file)\n",
        "def predict_next_word(model, context, n=2):\n",
        "    # Code to predict next word using an N-gram model\n",
        "    # For simplicity, return a dummy word\n",
        "    return \"example_next_word\"\n",
        "\n",
        "# Context sentence for prediction\n",
        "context = [\"machine\", \"learning\"]\n",
        "\n",
        "# Example usage of the N-gram model for word prediction\n",
        "predicted_word = predict_next_word(bigram_vectorizer, context, n=2) # Changed bigram_model to bigram_vectorizer\n",
        "print(f\"Predicted next word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2WRxL-Oa4z4",
        "outputId": "8448356f-9c96-45ec-9a62-a66e57f343f5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about egypt' 'absolutely loved' 'acting was' 'actors did'\n",
            " 'amazing movie' 'an amazing' 'and incredible' 'and loved' 'and wonderful'\n",
            " 'bad but' 'best films' 'boring but' 'but not' 'but the' 'did great'\n",
            " 'did not' 'disappointing it' 'egypt movie' 'ever seen' 'every part'\n",
            " 'fantastic and' 'fantastic plot' 'film it' 'film was' 'films have'\n",
            " 'great either' 'great job' 'great plot' 'hated the' 'have ever'\n",
            " 'have seen' 'highly recommend' 'in long' 'incredible performances'\n",
            " 'it about' 'it did' 'it was' 'job but' 'just okay' 'lacked depth'\n",
            " 'live up' 'long time' 'loved every' 'loved the' 'movie fantastic'\n",
            " 'movie regret' 'movie was' 'movie with' 'my time' 'not great' 'not live'\n",
            " 'not too' 'of it' 'of the' 'okay not' 'on it' 'one of' 'part of'\n",
            " 'plot and' 'recommend it' 'regret wasting' 'seen in' 'story lacked'\n",
            " 'storyline was' 'the acting' 'the actors' 'the best' 'the film'\n",
            " 'the hype' 'the movie' 'the story' 'the storyline' 'the worst'\n",
            " 'this film' 'time highly' 'time on' 'to the' 'too bad' 'up to'\n",
            " 'was boring' 'was brilliant' 'was disappointing' 'was fantastic'\n",
            " 'was just' 'was the' 'wasting my' 'with great' 'wonderful cast'\n",
            " 'worst have']\n",
            "Predicted next word: example_next_word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 8 Exploratory Questions on Task 4 & 5âœ…**\n"
      ],
      "metadata": {
        "id": "iwOF9nY6dd2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1-What types of entities (e.g., people, places, organizations) are most commonly identified in your text data?**"
      ],
      "metadata": {
        "id": "8hEXCHoBd-mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given dataset, the only named entity identified is \"Egypt\", which is classified as a GPE (Geopolitical Entity). No other people, places, or organizations were recognized, as the text mostly revolves around movie reviews and doesn't contain significant references to named entities beyond locations like Egypt."
      ],
      "metadata": {
        "id": "Xx3kWxeBeYbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2-How do these entities contribute to the overall meaning of the document?**\n"
      ],
      "metadata": {
        "id": "FrAjy_r6d87D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\"Egypt\" as a location suggests that the reviews might involve a movie related to Egypt. The entity contributes a specific context to the movie being reviewed, potentially shaping the audience's expectations or understanding of the movie's setting, which could be linked to themes around Egypt or Egyptian culture.\n"
      ],
      "metadata": {
        "id": "j5z16XPTezLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3-After performing POS tagging on your text, which parts of speech (e.g., nouns, verbs, adjectives) appear most frequently in the movie reviews?**"
      ],
      "metadata": {
        "id": "CWKiXBKce7so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Nouns such as \"movie\", \"film\", \"plot\", \"time\", \"story\", and \"actors\" appear frequently, as they are common in movie reviews to describe the subject matter.\n",
        "\n",
        "####Adjectives like \"fantastic\", \"boring\", \"brilliant\", and \"worst\" are also frequent, which is typical for reviews, as adjectives convey the writerâ€™s sentiment and evaluation of the film.\n",
        "\n",
        "####Verbs such as \"was\", \"loved\", \"hated\", \"recommend\", and \"seen\" are used to express opinions and describe actions related to the films."
      ],
      "metadata": {
        "id": "ITg-e5YRfBAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 9 Questions on Tasks 6 (TF-IDF) and 7 (N-gram)âœ…**"
      ],
      "metadata": {
        "id": "kLOGlTPudjLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1-What is the main purpose of applying TF-IDF in text processing?**\n"
      ],
      "metadata": {
        "id": "k3-F4vJRfzK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The main purpose of TF-IDF (Term Frequency-Inverse Document Frequency) is to convert the text into numerical features that reflect how important each word is within a specific document relative to the entire corpus. It helps in emphasizing words that are more relevant to a particular document while reducing the weight of frequently occurring words that are less informative (e.g., common stopwords)\n"
      ],
      "metadata": {
        "id": "_UMp3jFqgVOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2-How does the inverse document frequency (IDF) part of TF-IDF affect the importance of a word?**\n"
      ],
      "metadata": {
        "id": "O_HiQcXPf3Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The IDF part of TF-IDF reduces the weight of words that occur in many documents (common words) and increases the weight of words that are rare across the entire corpus but frequent in individual documents. This helps to highlight words that are more specific and meaningful to a particular document, giving them higher importance in the feature set.\n"
      ],
      "metadata": {
        "id": "XM2Ks3OBgW7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3-What are the possible issues that can arise if you do not apply stopword removal before calculating TF-IDF? (see my your experiment )**\n"
      ],
      "metadata": {
        "id": "whMZExoqf7TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Without removing stopwords, common words like \"the\", \"is\", \"and\" may be assigned higher TF-IDF values, even though they don't carry significant meaning. This can skew the results, causing the model to focus on irrelevant words and potentially reducing the performance of machine learning models that rely on these features."
      ],
      "metadata": {
        "id": "-Xjlub2KgaLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4-What is the difference between unigrams, bigrams, and trigrams?**\n"
      ],
      "metadata": {
        "id": "dD1HtfIAf-VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####- **Unigrams:** Single words (e.g., \"movie\").\n",
        "####- **Bigrams:** Consecutive sequences of two words (e.g., \"great movie\").\n",
        "####- **Trigrams:** Sequences of three consecutive words (e.g., \"fantastic movie plot\").\n",
        "\n",
        "####Each type captures a different level of context:\n",
        "####- **Unigrams** capture individual word importance.\n",
        "####- **Bigrams** and **capture relationships between words, useful for understanding context and dependencies in tasks like sentiment analysis or text classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HPVnewpTgdXB"
      }
    }
  ]
}